[{"title":"git&github常用操作","date":"2021-04-24T05:33:59.000Z","path":"/posts/20210424/","text":"git push项目到github：1234567891011git initgit add .git commit -m \"first commit\"git branch -M maingit remote add origin git@github.com:RidongHan/xxx.gitgit push origin main 其他命令123git statusgit reflog git-lfs 大文件支持1git lfs track \"*.zip\" document.querySelectorAll('.github-emoji') .forEach(el => { if (!el.dataset.src) { return; } const img = document.createElement('img'); img.style = 'display:none !important;'; img.src = el.dataset.src; img.addEventListener('error', () => { img.remove(); el.style.color = 'inherit'; el.style.backgroundImage = 'none'; el.style.background = 'none'; }); img.addEventListener('load', () => { img.remove(); }); document.body.appendChild(img); });","tags":[{"name":"Git/Github","slug":"Git-Github","permalink":"http://hanrd.tech/tags/Git-Github/"}]},{"title":"Linux之CPU主频控制","date":"2021-04-23T05:39:50.000Z","path":"/posts/20210423/","text":"最大频率降为70%1echo \"70\" | sudo tee /sys/devices/system/cpu/intel_pstate/max_perf_pct 查看效果1watch -n 0 \"cat /proc/cpuinfo | grep -i mhz\" document.querySelectorAll('.github-emoji') .forEach(el => { if (!el.dataset.src) { return; } const img = document.createElement('img'); img.style = 'display:none !important;'; img.src = el.dataset.src; img.addEventListener('error', () => { img.remove(); el.style.color = 'inherit'; el.style.backgroundImage = 'none'; el.style.background = 'none'; }); img.addEventListener('load', () => { img.remove(); }); document.body.appendChild(img); });","tags":[{"name":"Linux","slug":"Linux","permalink":"http://hanrd.tech/tags/Linux/"}]},{"title":"dpkg强制卸载软件","date":"2021-02-06T07:34:09.000Z","path":"/posts/20210206/","text":"dpkg强制卸载软件，忽略依赖warning1sudo dpkg --purge --force-depends 软件名 例如：1sudo dpkg --purge --force-depends \"libglvnd-dev\" document.querySelectorAll('.github-emoji') .forEach(el => { if (!el.dataset.src) { return; } const img = document.createElement('img'); img.style = 'display:none !important;'; img.src = el.dataset.src; img.addEventListener('error', () => { img.remove(); el.style.color = 'inherit'; el.style.backgroundImage = 'none'; el.style.background = 'none'; }); img.addEventListener('load', () => { img.remove(); }); document.body.appendChild(img); });","tags":[{"name":"dpkg","slug":"dpkg","permalink":"http://hanrd.tech/tags/dpkg/"}]},{"title":"COLING 2020 -- Improving Long-Tail Relation Extraction with Collaborating Relation-Augmented Attention","date":"2020-11-01T08:41:07.000Z","path":"/posts/20201101/","text":"摘要在关系抽取中，远程监督引起两个主要挑战： 错误标签 长尾关系（NYT数据集中，41个关系类别（共53个）只有不到1000个训练样例） 最近的工作通过“多实例学习的选择性注意力”减轻错误标签影响， 但即使引入关系的层次结构来共享知识，也不能很好地处理长尾关系。 为解决上述问题，本文提出一种新的神经网络，即协作关系增强的注意力（Collaborating Relation-augmented Attention，CoRA）。 具体如下： 首先提出关系增强注意力网络(relation-augmented attention network)，作为 base model。 bag级别的sentence-to-relation注意力机制，最大程度减少错误标记的影响。 基于 base model，引入在关系的层次结构中，各关系间共享的协同特征（collaborating relation features） 促进关系增强过程 平衡长尾关系的训练数据 主要训练目标: 预测句子bag的关系 辅助目标： 指导关系增强过程，以获得更准确的bag级特征表示 CoRA在数据集NYT上进行的实验在 $Precision@N$，$AUC$ 和 $Hits@K$ 指标上均达到SOTA。 对比实验的进一步分析也证明了CoRA在处理长尾关系方面的卓越能力。 引言在关系抽取中，远程监督引起两个主要挑战： 错误标签 长尾关系（NYT数据集中，41个关系类别（共53个）只有不到1000个训练样例） 对于错误标签问题： 最近的工作大多是通过“多实例学习的选择性注意力”减轻错误标签影响 对于long-tail问题： document.querySelectorAll('.github-emoji') .forEach(el => { if (!el.dataset.src) { return; } const img = document.createElement('img'); img.style = 'display:none !important;'; img.src = el.dataset.src; img.addEventListener('error', () => { img.remove(); el.style.color = 'inherit'; el.style.backgroundImage = 'none'; el.style.background = 'none'; }); img.addEventListener('load', () => { img.remove(); }); document.body.appendChild(img); });","tags":[{"name":"Relation Extraction","slug":"Relation-Extraction","permalink":"http://hanrd.tech/tags/Relation-Extraction/"},{"name":"COLING 2020","slug":"COLING-2020","permalink":"http://hanrd.tech/tags/COLING-2020/"}]},{"title":"ICASSP 2016 -- Training deep neural-networks based on unreliable labels","date":"2020-10-22T02:38:58.000Z","path":"/posts/20201022/","text":"摘要本文拟解决基于 “带有不可靠标签的数据” 训练神经网络的问题。 基于假设观察到的标签是真实标签的带噪观测结果，从而引入一个额外的噪音层，模拟噪音分布对真实标签的影响。 提出一种同时学习神经网络参数和噪声分布的方法 在几个标准分类任务上的对比实验说明了该方法对性能的提升。 在某些情况下，即使“人工手动标注并假定没有错误的数据”，该方法也有所帮助。 模型 假设在训练过程中不能直接观察到正确标签y，只能观察到带噪标签z 噪音分布可以看作关系标签间的转移概率 $\\theta (i,j) = p(z=j|y=i) $ 观测的带噪标签 z 的概率： $ p(z=j|x;\\omega, \\theta) = \\sum\\limits_{i=1}^k p(z=j|y=i; \\theta) p(y=i|x;\\omega), $ $\\omega是参数集合, x是输入特征（可以是句子、手工创建的特征等）, k是关系的个数$ 模型结构图如下： model 对于输入特征 $x$ , 编码后的特征向量表示为 $h = h(x)$, $p(y=i|x;\\omega) = \\frac {\\exp(u_i^\\top h)}{\\sum\\limits_{j=1}^k \\exp(u_j^\\top h)}, i=1,2,…,k$ $u$ 是参数 训练阶段 给定 n 个输入特征 $x_1,…,x_n$, 相对应的观测带噪标签 $z_1,…, z_n$ , 真实标签 $y_1,…,y_n$ 对数极大似然估计： $L(\\omega, \\theta) = \\sum\\limits_{t=1}^n \\log (\\sum\\limits_{i=1}^k p(z_t|y_t=i;\\theta) p(y_t=i|x_t;\\omega))$ 目标是：最大化该似然函数，找出参数 $\\omega$ 、噪音分布 $\\theta$ 由于 Improving Long-Tail Relation Extraction with Collaborating Relation-Augmented Attention document.querySelectorAll('.github-emoji') .forEach(el => { if (!el.dataset.src) { return; } const img = document.createElement('img'); img.style = 'display:none !important;'; img.src = el.dataset.src; img.addEventListener('error', () => { img.remove(); el.style.color = 'inherit'; el.style.backgroundImage = 'none'; el.style.background = 'none'; }); img.addEventListener('load', () => { img.remove(); }); document.body.appendChild(img); });","tags":[{"name":"Relation Extraction","slug":"Relation-Extraction","permalink":"http://hanrd.tech/tags/Relation-Extraction/"},{"name":"ICASSP 2016","slug":"ICASSP-2016","permalink":"http://hanrd.tech/tags/ICASSP-2016/"}]},{"title":"软件学报2020--基于带噪观测的远监督神经网络关系抽取","date":"2020-10-20T14:01:19.000Z","path":"/posts/20201020/","text":"拟解决问题远程监督关系抽取的标记噪音问题 出发点提出”远程监督中，最终句子对齐的标签是基于某些未知因素所生成的带噪观测结果”这一假设。 模型模型结构如下图所示： 带噪观测模型 document.querySelectorAll('.github-emoji') .forEach(el => { if (!el.dataset.src) { return; } const img = document.createElement('img'); img.style = 'display:none !important;'; img.src = el.dataset.src; img.addEventListener('error', () => { img.remove(); el.style.color = 'inherit'; el.style.backgroundImage = 'none'; el.style.background = 'none'; }); img.addEventListener('load', () => { img.remove(); }); document.body.appendChild(img); });","tags":[{"name":"Relation Extraction","slug":"Relation-Extraction","permalink":"http://hanrd.tech/tags/Relation-Extraction/"},{"name":"软件学报 2020","slug":"软件学报-2020","permalink":"http://hanrd.tech/tags/%E8%BD%AF%E4%BB%B6%E5%AD%A6%E6%8A%A5-2020/"}]},{"title":"ACL 2020 - A Novel Cascade Binary Tagging Framework for Relational Triple Extraction","date":"2020-10-06T03:13:58.000Z","path":"/posts/20201006/","text":"摘要现有工作很少能够较好地解决关系三元组的重叠问题（overlapping triple problem）。 本文用一个新的视角来重新审视关系提取任务，并提出了一种级联二分标注框架(Cascade Binary Tagging Framework, 简称CasRel)。 该框架将关系（relation）建模为主体（subject）与客体（object）的函数映射，而不是像以前的工作那样将关系视为离散标签。 实验表明，该框架在多个场景下均可以获得性能提升，某些场景大幅度超越现有SOTA。 拟解决问题重叠三元组问题overlapping triple problem有以下三种情形： 本文方案本文关系抽取任务定义 本文的框架将关系（relation）建模为主体（subject）与客体（object）的函数映射，而不是像以前的工作那样将关系视为离散标签。 由学习关系分类器（relation为离散关系标签） $f(s,o) \\to r$ 转变为学习主体s在关系r下对客体o的函数映射 $f_r(s) \\to o$ 基于上述视角，关系抽取任务可以分为以下两个步骤： 确定句子中所有可能的主体 (subject)； 针对每个主体subject，使用特定于关系的标注器（relation-specific tagger）同时识别所有可能的关系和相应的客体object CasRel框架（三部分）： BERT-based encoder module (基于Bert的编码器模块) subject tagging module(主体标注模块) relation-specific object tagging module(区分关系的客体标注模块) 模型结构 训练目标针对训练集 $D$ 中的句子$x_j$，给定对应的潜在overlap triples集合 $ T_j = \\{(s,r,o)\\} $，最大化数据似然估计： $ \\prod\\limits_{j=1}^{|D|} {\\left[ \\prod\\limits_{(s,r,o) \\in T_j} {p((s,r,o)|x_j)}\\right]} $ $ = \\prod\\limits_{j=1}^{|D|} {\\left[ \\prod\\limits_{s \\in T_j} p(s|x_j) \\prod\\limits_{(r,o) \\in T_j|s} p((r,o)|s,x_j)\\right]}$ $ = \\prod\\limits_{j=1}^{|D|} {\\left[ \\prod\\limits_{s \\in T_j} p(s|x_j) \\prod\\limits_{ r \\in T_j|s} p_r(o|s,x_j) \\prod\\limits_{r \\in R \\backslash T_j|s} p_r(o_{\\emptyset}|s,x_j)\\right]}$ 其中，$s \\in T_j$ 表示出现在 $T_j$ 中三元组的主体（subject）， $T_j | s $ 表示 $T_j$ 中主体是 $s$ 的三元组集合， $R$ 是所有可能的关系的集合， $\\backslash$ 表示集合的差集, $o_\\emptyset$ 表示”null”客体。 对于任一个给定的主体 s , 在”s确实参与表达的关系r”的作用下，必定可以映射为客体 o ,而对于其他关系，映射为”空”客体 $null$ BERT Encoder编码器使用Bert Subject Tagger训练两个二元分类器，分别识别Subject的开始位置和结束位置,标识为1, 其他标识为0 对于每一个起始位置，从该位置依次向后寻找最近的结束位置, 从而寻找出所有Subject $p_i^{start_s} = \\sigma (W_{start} X_i + b_{start})$, $p_i^{end_s} = \\sigma (W_{end} X_i + b_{end})$, $ X_i $ 表示序列第i个词的bert编码， $\\sigma $ 表示sigmoid激活函数。 loss最大化似然函数：$p_\\theta(s|X) = \\prod\\limits_{t \\in \\{start_s,end_s\\}} {\\prod\\limits_{i=1}^L (p_i^t)^{I\\{y_i^t = 1\\} }(1-p_i^t)^{I\\{y_i^t = 0\\}}}$, $L$是句子长度， $I\\{z\\} = 1$ ，如果 z 为 true， 否则 0， $\\theta$ 是参数集合 经过这一步骤，可是识别出句子中所有Subject Relation-specific Object Taggers针对每一个关系 $r$,训练两个二元分类器，分别识别针对关系r映射出的Object的开始位置和结束位置，标识为1， 否则标识为0 $p_i^{start_o} = \\sigma (W_{start}^r (X_i + V^k_{sub}) + b_{start}^r)$, $p_i^{end_o} = \\sigma (W_{end}^r (X_i + V^k_{sub}) + b_{end}^r)$, $V_{sub}^k$ 表示subject tagger识别出的第k个Subject的编码，若为多个词构成，取均值 loss最大化似然函数： $p_{\\emptyset_r}(o|s, X) = \\prod\\limits_{t \\in \\{start_o,end_o\\}} {\\prod\\limits_{i=1}^L (p_i^t)^{I\\{y_i^t = 1\\} }(1-p_i^t)^{I\\{y_i^t = 0\\}}}$, $L$是句子长度， $I\\{z\\} = 1$ ，如果 z 为 true， 否则 0， $\\emptyset_r$ 是参数集合, 另外，“空”客体Object，意味着 y_i^{start\\_o_{\\emptyset}} = y_i^{end\\_o_{\\emptyset}} = 0 针对Subject Tagger识别出的每一个Subject，计算该subject在每个关系作用下的Object。对于每一个关系 $r$若得到非空Object，既可以抽取出关系三元组，若得到空Object，则认为不存在该关系。 整体Loss最大化似然函数： $J(\\Theta) = \\sum\\limits _{j=1} ^ {|D|} \\left[ {\\sum\\limits _{s \\in T_j} \\log p_\\theta(s|X_j)} + {\\sum\\limits _{r \\in T_j|s} \\log p_{\\emptyset_r}(o|s,X_j)} + {\\sum\\limits _{r \\in R \\backslash T_j|s} \\log p_{\\emptyset_r}(o_\\emptyset|s,X_j)} \\right]$ document.querySelectorAll('.github-emoji') .forEach(el => { if (!el.dataset.src) { return; } const img = document.createElement('img'); img.style = 'display:none !important;'; img.src = el.dataset.src; img.addEventListener('error', () => { img.remove(); el.style.color = 'inherit'; el.style.backgroundImage = 'none'; el.style.background = 'none'; }); img.addEventListener('load', () => { img.remove(); }); document.body.appendChild(img); });","tags":[{"name":"Relation Extraction","slug":"Relation-Extraction","permalink":"http://hanrd.tech/tags/Relation-Extraction/"},{"name":"ACL 2020","slug":"ACL-2020","permalink":"http://hanrd.tech/tags/ACL-2020/"}]},{"title":"主题模型","date":"2020-09-21T05:38:41.000Z","path":"/posts/20200921-TopicModel/","text":"主题模型 （Topic Model） 主题模型（Topic Model）是以无监督学习的方式对文档的隐含语义结构(latent semantic structure)进行聚类(clustering)的统计模型。 主题模型认为在词(word)与文档(document)之间没有直接的联系，应当还有一个维度将它们串联起来，这个维度称为主题(topic)。 每个文档都对应着一个或多个主题，而每个主题都有对应的词分布，通过主题，就可以得到每个文档的词分布。 由此有公式： p(\\omega_i | d_j) = \\sum _{k=1} ^{K} p(\\omega_i | t_k) \\times p( t_k | d_j), 其中\\omega表示词，d表示文档，t表示主题，K表示主题个数 在一个已知的数据集中，词和文档对应的$p(\\omega_i | d_j)$都是已知的。主题模型就是根据这个已知的信息，通过计算$p(\\omega_i | t_k)$和$p( t_k | d_j)$，从而得到主题的词分布和文档的主题分布信息。 常用方法 LSA（Latent Semantic Analysis） 主要采用SVD（奇异值分解）暴力破解 LDA（Latent Dirichlet Allocation, 隐含狄利克雷分布） 贝叶斯学派方法论进行拟合 LSA（Latent Semantic Analysis）LSA最初用在语义检索上，为解决一词多义和一义多词的问题： 一义多词： 美女和PPMM表示相同的含义，但是单纯依靠检索词“美女”来检索文档，很可能丧失掉那些包含“PPMM”的文档。 一词多义： 如果输入检索词是多个检索词组成的一个小document，例如“清澈 孩子”，那我们就知道这段文字主要想表达concept是和道德相关的，不应该将“春天到了，小河多么的清澈”这样的文本包含在内。 为了能够解决这个问题，需要将词语（term）中的concept提取出来，建立一个词语和概念的关联关系（t-c relationship），这样一个文档就能表示成为概念的向量。这样输入一段检索词之后，就可以先将检索词转换为概念，再通过概念去匹配文档 LDA (Latent Dirichlet Allocation)隐含狄利克雷分布（Latent Dirichlet Allocation, LDA）是由David Blei等人在2003年提出的，该方法的理论基础是贝叶斯理论。 LDA根据词的共现信息，拟合出词-文档-主题的分布，进而将词、文本都映射到一个语义空间中。 LDA算法假设文档中主题的先验分布和主题中词的先验分布都服从狄利克雷分布。在贝叶斯学派看来，先验分布+数据(似然)=后验分布。我们通过对已有数据集的统计，就可以得到每篇文档中主题的多项式分布和每个主题对应词的多项式分布。然后就可以根据贝叶斯学派的方法，通过先验的狄利克雷分布和观测数据得到的多项式分布，得到一组Dirichlet-multi共轭，并据此来推断文档中主题的后验分布，也就是我们最后需要的结果。那么具体的LDA模型应当如何进行求解，其中一种主流的方法就是吉布斯采样。结合吉布斯采样的LDA模型训练过程一般如下： 随机初始化，对语料中每篇文档中的每个词w，随机地赋予一个topic编号z。重新扫描语料库，对每个词w按照吉布斯采样公式重新采样它的topic，在语料中进行更新。重复以上语料库的重新采样过程直到吉布斯采样收敛。统计语料库的topic-word共现频率矩阵，该矩阵就是LDA的模型。经过以上的步骤，就得到一个训练好的LDA模型，接下来就可以按照一定的方式针对新文档的topic进行预估，具体步骤如下： 随机初始化，对当前文档中的每个词w，随机地赋予一个topic编号z。重新扫描当前文档，按照吉布斯采样公式，重新采样它的topic。重复以上过程直到吉布斯采样收敛。统计文档中的topic分布即为预估结果。 document.querySelectorAll('.github-emoji') .forEach(el => { if (!el.dataset.src) { return; } const img = document.createElement('img'); img.style = 'display:none !important;'; img.src = el.dataset.src; img.addEventListener('error', () => { img.remove(); el.style.color = 'inherit'; el.style.backgroundImage = 'none'; el.style.background = 'none'; }); img.addEventListener('load', () => { img.remove(); }); document.body.appendChild(img); });","tags":[{"name":"Topic Model","slug":"Topic-Model","permalink":"http://hanrd.tech/tags/Topic-Model/"},{"name":"主题模型","slug":"主题模型","permalink":"http://hanrd.tech/tags/%E4%B8%BB%E9%A2%98%E6%A8%A1%E5%9E%8B/"}]},{"title":"AAAI 2020 -- Self-Attention Enhanced Selective Gate with Entity-Aware Embedding for Distantly Supervised Relation Extraction","date":"2020-09-16T12:15:08.000Z","path":"/posts/20200916/","text":"摘要 远程监督由于其强烈的假设，深受噪音数据标签的影响。 大多数现有工作都在bag级别采用选择性注意力机制来降噪。但是其无法胜任单句子bag的情况。 本文提出： 提出一种实体感知词嵌入方法(entity-aware word embedding),来整合位置信息和头/尾实体embedding，以突出此任务的实体本质； 以PCNN捕获局部依存关系，并提出一种自注意力机制（self-attention）来捕获全局依存关系，作为PCNN的补充； 设计一个基于池的门(pooling-equipped gate)，来代替选择性注意力机制，该门基于上下文表示，作为聚合器生成bag级别的表示。 与选择性注意力机制相比，该门控机制的主要优点是，即使bag中只有一个句子，也可以稳定执行，从而在所有训练数据中保持一致。 在NYT 2010数据集上实验表明，该模型在AUC和top-n指标方面均达到SOTA性能。 拟解决的问题 单句子bag在实际数据集上占比很高，并且单句子bag的标签很可能是错误的，即噪音bag； 单句子bag的情况，会迫使基于选择性注意力机制的模型，返回单值标量的权重，导致注意力模块不能充分训练，从而影响性能。 创新点 使用实体嵌入(entity embedding)和相对位置嵌入(position embedding)，并提出一种实体感知嵌入方法(entity-aware embedding approach)将实体信息动态集成到每个单词嵌入中，产生更具表达力的表示; 为增强PCNN捕获长期依赖的能力（Yu等人，2018），开发一种轻量级的自注意机制来捕获丰富的依赖信息，为PCNN产生互补依赖表示; 设计一个选择性门结构（selective gate），将句子级别表示聚合为bag级别的表示。 Model(注: 本人认为本篇论文中的公式描述和模型图片都有点错误) 结构图 Entity-Aware Embedding计算过程如下： 1.首先针对bag中的每一个句子： $s= [\\omega_1,\\omega_2,…,\\omega_n]$, 词嵌入表示为 $X^{(\\omega)} = [v_1,v_2,…,v_n] \\in R^{d_\\omega \\times n}$ 2.加入相对位置嵌入(相对距离), 位置嵌入表示为 $ r_i^{e_h} 、r_i^{e_t} \\in R^{d_r}$： $ X^{(p)} = [x_1^{(p)}, x_2^{(p)},…,x_n^{(p)}] \\in R^{d_p \\times n} , \\forall x_i^{(p)} = [v_i; r_i^{e_h}; r_i^{e_t}], 其中d_p = d_\\omega + 2 \\times d_r$ 3.加入实体嵌入, 头尾实体嵌入表示为$v^{(h)}、v^{(t)}$： $X^{(e)} = [x_1^{(e)}, x_2^{(e)},…,x_n^{(e)}] \\in R^{3d_\\omega \\times n}, \\forall x_i^{(e)} = [v_i; v^{(h)};v^{(t)}] \\in R^{3d_\\omega}$ 4.gate机制计算： $ \\alpha=sigmoid(\\lambda \\cdot (W^{(g1)}X^{(e)} + b^{(g1)})) \\in R^{d_h \\times n}, W^{(g1)} \\in R^{d_h \\times 3d_\\omega}, \\lambda是超参数$ $\\tilde{X}^{(p)} = \\tanh(W^{(g2)}X^{(p)} + b^{(g2)}) \\in R^{d_h \\times n}, W^{(g2)} \\in R^{d_h \\times d_p}$ $X =\\alpha \\cdot (W^{(X)} X^{(e)}+ b^{(X)}) + (1-\\alpha) \\cdot \\tilde X ^{(p)} \\in R^{d_h \\times n}, 其中W^{(X)} \\in R^{d_h \\times 3d_\\omega}$ 得到的$X$,被认为是所有单词的entity-aware embedding。 self-Attention Enhanced Neural Network 之前的一些工作（Vaswani等人，2017）发现，CNN由于缺乏衡量长期依赖的能力，即使堆叠多层，也无法在大多数NLP Benchmarks上达到SOTA性能。 PCNN针对每一个句子的计算公式 一维卷积操作可以表示为： $ H^{(c)} = 1D_CNN(X;W^{(c)}; b^{(c)}) \\in R^{d_c \\times n}, d_c是输出通道个数(卷积核个数)$ 所得output根据头尾实体位置划分： $H^{(c)} = [H^{(1)},H^{(2)},H^{(3)}]$ $s = \\tanh([Pool(H^{(1)}); Pool(H^{(2)}); Pool(H^{(3)})]) \\in R^{3d_c}$ Self-Attention Mechanism计算公式: $A = W^{(\\alpha2)} \\sigma(W^{(\\alpha1)}X + b^{(\\alpha1)}) + b^{(\\alpha2)}, 其中W^{(\\alpha1)}、W^{(\\alpha2)} \\in R^{d_h \\times d_h}, \\sigma 是激活函数$ $P^{(A)} = softmax(A) \\in R^{d_h \\times n}$ $u = \\sum P^{(A)} \\odot X \\in R^{d_h}$ Selective Gate给定一个包含m个句子的bag，通过PCNN、Self-Attention得到的句子表示为 $S = [s_1,s_2,…,s_m] \\in R^{3d_c \\times m}$, $U = [u_1,u_2,…,u_m] \\in R^{d_h \\times m}$ 计算公式如下： 1.首先逐句生成gate值，该值控制该句子的表示是否应该被保留 $g_j = sigmoid(W^{(g1)} \\sigma(W^{(g2)}u_j + b^{(g2)}) + b^{(g1)}) \\in R^{3d_c}, \\forall j=1,…,m$ $W^{(g1)} \\in R^{3d_c \\times d_h}, W^{(g2)} \\in R^{d_h \\times d_h} , \\sigma(\\cdot)是激活函数$ 2.对PCNN的句子表示加权求和 $c = \\frac 1 m \\sum_{j=1}^m {g_j \\cdot s_j} \\in R^{3d_c}$ Output$p = softmax(MLP(c)) \\in R^{|C|}, 其中|C|是关系种类$ LOSSL2正则化 $L_{NLL} = - \\frac 1 {|D|} \\sum_{k=1}^{|D|} \\log p_{(y^k)}^k + \\beta || {\\theta} || _2^2, 其中\\theta表示模型所有参数，\\beta是系数$ document.querySelectorAll('.github-emoji') .forEach(el => { if (!el.dataset.src) { return; } const img = document.createElement('img'); img.style = 'display:none !important;'; img.src = el.dataset.src; img.addEventListener('error', () => { img.remove(); el.style.color = 'inherit'; el.style.backgroundImage = 'none'; el.style.background = 'none'; }); img.addEventListener('load', () => { img.remove(); }); document.body.appendChild(img); });","tags":[{"name":"AAAI 2020","slug":"AAAI-2020","permalink":"http://hanrd.tech/tags/AAAI-2020/"},{"name":"Relation Extraction","slug":"Relation-Extraction","permalink":"http://hanrd.tech/tags/Relation-Extraction/"}]},{"title":"ACL 2020 -- Relation Extraction with Explanation","date":"2020-09-05T03:04:15.000Z","path":"/posts/20200905/","text":"摘要 最近远程监督关系抽取的NRE模型都是针对每个分包bag，通过学习句子的重要性权重来减轻不相关句子的影响。目前为止，努力的重点是提高抽取的准确性，但对模型的解释性知之甚少。 本文标注了一个带有基本事实句子级解释的测试集，以评估关系提取模型提供的可解释性。实验证明，用细粒度实体类型（entity type）替换句子中的实体不仅提高了提取的准确性，而且改善了可解释性。本文还提出**自动生成“干扰”句子，对bag数据增强，并训练模型忽略这些“干扰”句子。在FB-NYT数据集上，本文取得了SOTA性能，同时提高了模型可解释性。 引言远程监督关系抽取降噪研究： 基于attention的模型 利用附加资源（additional resources）的模型 利用监督数据的模型 现有方法，关注于提高模型的准确率（accuracy），而对于模型是因为正确的推理还是某些不相关的偏置/偏见而正确分类知之甚少。 Baseline ModelsDirectSup给定一个分包bag，DirectSup使用具有不同过滤器大小的CNN对每个句子进行编码。连接具有不同滤波器大小的CNN的输出，以产生句子的编码。 CNNs+ATT document.querySelectorAll('.github-emoji') .forEach(el => { if (!el.dataset.src) { return; } const img = document.createElement('img'); img.style = 'display:none !important;'; img.src = el.dataset.src; img.addEventListener('error', () => { img.remove(); el.style.color = 'inherit'; el.style.backgroundImage = 'none'; el.style.background = 'none'; }); img.addEventListener('load', () => { img.remove(); }); document.body.appendChild(img); });","tags":[{"name":"Relation Extraction","slug":"Relation-Extraction","permalink":"http://hanrd.tech/tags/Relation-Extraction/"},{"name":"ACL 2020","slug":"ACL-2020","permalink":"http://hanrd.tech/tags/ACL-2020/"}]},{"title":"VS code - Module 'torch' has no 'xxx' member","date":"2020-09-04T03:05:05.000Z","path":"/posts/20200904a/","text":"vscode ‘torch’ has no ‘xxx’ membervscode User settings中加上 \"python.linting.pylintArgs\": [ \"–errors-only\", \"–generated-members=numpy.* ,torch.* ,cv2.* , cv.*\", ] document.querySelectorAll('.github-emoji') .forEach(el => { if (!el.dataset.src) { return; } const img = document.createElement('img'); img.style = 'display:none !important;'; img.src = el.dataset.src; img.addEventListener('error', () => { img.remove(); el.style.color = 'inherit'; el.style.backgroundImage = 'none'; el.style.background = 'none'; }); img.addEventListener('load', () => { img.remove(); }); document.body.appendChild(img); });","tags":[{"name":"VS code","slug":"VS-code","permalink":"http://hanrd.tech/tags/VS-code/"}]},{"title":"markdownPad2 -- 破解","date":"2020-09-02T11:57:49.000Z","path":"/posts/20200902a/","text":"Email address : Soar360@live.com License key : GBPduHjWfJU1mZqcPM3BikjYKF6xKhlKIys3i1MU2eJHqWGImDHzWdD6xhMNLGVpbP2M5SN6bnxn2kSE8qHqNY5QaaRxmO3YSMHxlv2EYpjdwLcPwfeTG7kUdnhKE0vVy4RidP6Y2wZ0q74f47fzsZo45JE2hfQBFi2O9Jldjp1mW8HUpTtLA2a5/sQytXJUQl/QKO0jUQY4pa5CCx20sV1ClOTZtAGngSOJtIOFXK599sBr5aIEFyH0K7H4BoNMiiDMnxt1rD8Vb/ikJdhGMMQr0R4B+L3nWU97eaVPTRKfWGDE8/eAgKzpGwrQQoDh+nzX1xoVQ8NAuH+s4UcSeQ== document.querySelectorAll('.github-emoji') .forEach(el => { if (!el.dataset.src) { return; } const img = document.createElement('img'); img.style = 'display:none !important;'; img.src = el.dataset.src; img.addEventListener('error', () => { img.remove(); el.style.color = 'inherit'; el.style.backgroundImage = 'none'; el.style.background = 'none'; }); img.addEventListener('load', () => { img.remove(); }); document.body.appendChild(img); });","tags":[{"name":"MarkdownPad2","slug":"MarkdownPad2","permalink":"http://hanrd.tech/tags/MarkdownPad2/"}]},{"title":"ICLR 2019 -- ORDERED NEURONS:INTEGRATING TREE STRUCTURES INTO RECURRENT NEURAL NETWORKS","date":"2020-08-30T08:28:30.000Z","path":"/posts/20200830a/","text":"拟解决问题语言虽然看起来是一个序列，实际上内部是有复杂的层次结构的，这也是NLP的难点所在。复杂的层次结构，意味着序列即使看起来相同，也可能因为内部层次结构的不同而有语义的差别。 在斯坦福CS224n上提到了这样的一个例子： The police killed the man with a knife. 这个句子，可以有两种理解： 警察把那个带刀的人干掉了 警察用刀干掉了那个人 上面两种解释对应的句法树分别是这样的： 句法树 序列看起来一样，但是由于内部的层级结构不一样会导致不同的语义理解 当用LSTM为语言序列编码Encode的时候，由于LSTM单纯认为语言是一个序列，忽略了语言内部的语法树的层级结构，因此其无法解决上面相同序列，不同语义的问题。 出发点本文提出ON-LSTM(Ordered Neurons - LSTM)，通过重新设计LSTM递归的cell的cell states的更新方式及策略，实现将语法树的层级结构融合进LSTM编码器中，解决上述问题。 回顾LSTMLSTM主要由三个特殊的门（gate）结构组成：遗忘门、输入门、输出门 具体的公式如下： 示意图如下： ON-LSTM从LSTM开始论文中一直是从神经元排序的角度解释的，对我个人来说，很难理解，故下文按照cell state的角度理解。 LSTM的核心就是cell state：信息在cell state这个传送带上流动，伴随着一些简单的线性变换，乘和加，分别由“遗忘门”和“输入门”来控制cell state的信息更新。 这样存在一个问题：每次更新，cell state这个向量的每一维都会更新 信息流就是存在于这个cell state中，如果希望模型可以刻画出语言的结构信息，也就意味着这个cell state中要隐含着层次结构的信息。 所以作者希望，能够让这个cell state的不同维度，对应到语言的不同层级上，让不同的层级使用不一样的方式进行更新，具体来说就是层次越高的更新越少。这样的话，cell state就包含了层次信息了。 作者的例子： 假设我们有一个很简单的句子，三个词组成[x1,x2,x3]，有三个层次，用下图的最左边的图表示，分别是句子（S）、短语（NP，VP）、词（N，V）。我们希望cell state中也可以有对应的三个层次，层次就体现在不同的更新频率上。 层次越高的，自然其信息应该保留的时间更久，所以其更新频率应该越低。上图的最右边是三个词分别对应的cell states。颜色越深代表更新频率越高。 当读到第一个词x1的时候，这个时候是new S，new NP，new word，所以三个层次都应该更新； 当读到第二个词x2的时候，这个时候是new VP，new word，所以下面两个层次应该更新； 当读到第三个词x3的时候，这个时候只是new word，所以只是最下面的层次应该更新；这样，语言的层次就和cell states的不同区间对应上的。 这样，就相当于给cell states加了一个顺序，从某种意义上讲也相当于是给LSTM的神经元加了顺序，因此作者称这种结构是Ordered-Neurons，对应的LSTM称为ON-LSTM。所以并不是真的给神经元排序。 接下来的问题： 上面给cell states这样分区间，是因为我们提前知道了句子的结构，但我们真正使用LSTM进行建模、训练的时候，是不知道语言的真实层次的，除非你先把每个句子都解析成语法树，再显式加入到LSTM中的，但是这种方法不仅开销大，而且不一定可靠，所以我们需要设计一种结构，让模型可以学习到如何给cell state去分区。 ON-LSTM具体设计为了实现区间的划分，模型用到了两个整数 $l_{his}$ 和 $l_{now}$，它们分别用来表示历史信息的最低等级 和 当前信息的最高等级 $l_{his}$表示cell state中高于该等级的维度需要保留历史信息$c_{t-1}$，对应于低更新频率 $l_{now}$表示cell state中低于该等级的维度需要补充更新输入信息$\\hat{c_t}$，对应于高更新频率 总体上，就包含以下两种情况： $l_{his} &lt; l_{now}$ ,有重叠交汇部分： 交汇部分，采用原来LSTM更新方式，融合历史信息和当前输入信息 低于$l_{his}$的，完全更新为新输入的信息 高于$l_{now}$的，完全保留历史信息 $l_{his} &gt; l_{now}$ ,无重叠交汇部分： 无交汇部分，全部设置为0 低于$l_{now}$的，完全更新为新输入的信息 高于$l_{his}$的，完全保留历史信息 其实该模型认为高层次的语法信息主要是来自于历史信息，而低层次的主要来自当前输入信息，而这也比较符合人们的直观印象，对于一个新的输入，它对于语法信息的影响往往局限于一个较低的层次，高层次的信息（如句子或者短语信息）仍然来自于历史信息，只有当一个句子或者短语完结的时候，历史信息的影响变小，这时新的输入才有可能影响较高语法层次的信息。而这样也就使得高语法层次的信息的更新频率较低，大多时候是保持不变，而低语法层次的信息则随着当前的输入一直变化。 ON-LSTM实现过程把$l_{his}、 l_{now}$转化为向量 L_k = [0,0,...,1,0,...,0] ,其中只有第k位为1定义累加函数 cumsum([x_1,x_2,...,x_n]) = [x_1, x_1+x_2, x_1+x_2+x_3,..., x_1+x_2+...+x_n]因此， $cumsum(L_{l_{his}})=[0,…,0,1,1,…,1]$就表示出了需要保留历史信息的维度（1段） $1-cumsum(L_{l_{now}})=[1,…,1,0,0,…,0]$就表示出了需要更新保存新输入信息的维度（1段） 由于，上面这种分为0段和1段的形式，都是整数，这样函数不可导，无法训练，所以需要做一下软化，即用softmax函数处理一下。 定义 $cummax(X) = cumsum(softmax(X))$ 综上，引入两个门（gate）结构：master forget（$\\tilde{f_t}$）、master input（$\\tilde{i_t}$）。 具体计算公式如下： \\tilde{f_t} = cummax(W_{\\tilde{f_t}}x_t+U_{\\tilde{f_t}}h_{t-1}+b_{\\tilde{f_t}})\\tilde{i_t} = 1-cummax(W_{\\tilde{i_t}}x_t+U_{\\tilde{i_t}}h_{t-1}+b_{\\tilde{i_t}})\\omega_t=\\tilde{f_t} \\circ \\tilde{i_t} (按位相乘，表示重叠交汇部分)c_t = \\omega_t \\circ (f_t \\circ c_{t-1}+ i_t \\circ \\hat{c_t}) + (\\tilde{f_t}-\\omega_t) \\circ c_{t-1} + (\\tilde{i_t}-\\omega_t) \\circ \\hat{c_t}整体计算流程如下： 通常隐层神经元的数目都比较大，而实际中语法的层数远远达不到这个数字，因此对于$\\tilde{f_t}$和$\\tilde{i_t}$而言，其实不需要那么多的维数，这样会导致需要学习的参数量过多，但是 $\\circ$ 要求它们的维数必须这么大，因此我们可以构造一个维数为$D_m = D / c$ 的向量，其中D为隐层神经元的维数，然后在将其扩充为D维向量，例如D=6，c=3，先构造一个向量[0.2, 0.8] ，然后将其扩充为 [0.2, 0.2, 0.2, 0.8, 0.8, 0.8] 。 总结该ON-LSTM模型从语法结构的角度出发，根据语法层次对cell states进行有序排列，再按照语法层次的不同实行不同的更新规则，从而实现对于较高语法层次信息的保留，这样对于语言模型等任务无疑是很有利的。另外，利用该模型还能够较好地从句子中无监督地提取出语法结构，而这也是该模型的一大亮点。 参考资料[1] ORDERED NEURONS: INTEGRATING TREE STRUCTURES INTO RECURRENT NEURAL NETWORKS [2] 苏剑林. (2019, May 28). ON-LSTM：用有序神经元表达层次结构 [Blog post]. [3] 有序的神经元——ON-LSTM模型浅析 [4] ON-LSTM：能表示语言层次的LSTM document.querySelectorAll('.github-emoji') .forEach(el => { if (!el.dataset.src) { return; } const img = document.createElement('img'); img.style = 'display:none !important;'; img.src = el.dataset.src; img.addEventListener('error', () => { img.remove(); el.style.color = 'inherit'; el.style.backgroundImage = 'none'; el.style.background = 'none'; }); img.addEventListener('load', () => { img.remove(); }); document.body.appendChild(img); });","tags":[{"name":"ICLR 2019","slug":"ICLR-2019","permalink":"http://hanrd.tech/tags/ICLR-2019/"}]},{"title":"ACL 2020 -- Exploiting the Syntax-Model Consistency for Neural Relation Extraction","date":"2020-08-29T06:02:41.000Z","path":"/posts/20200829a/","text":"摘要本文研究了关系抽取(Re)的任务，目的是识别文本中提到的两个实体之间的语义关系。 在Re的深度学习模型中，整合输入句子的依存树中的句法结构是有益的。在这种模型中，依赖树经常被用来直接构造网络结构或获得单词对之间的依赖关系，以此通过多任务学习将句法信息注入模型中。 这些方法的主要问题是训练数据中缺乏超越句法结构的泛化，或者未能捕捉词对于RE的句法重要性。 为了克服这些问题，我们提出了一种新的Re深度学习模型，该模型使用依赖树提取单词的基于句法的重要性得分，作为树的向量表示，以将句法信息注入具有更大泛化性的模型中。 特别是，我们利用有序神经元长短期记忆网络(ON-LSTM)来推断句子中每个单词的基于模型的重要性得分，然后对其进行调整，使其与基于句法的分数保持一致，从而实现句法信息注入。 我们进行了广泛的实验来证明所提出的方法的有效性，从而在三个基准数据集上取得了SOTA性能。 引言最近对RE的研究集中在深度学习上，以开发从文本数据中自动生成句子向量表示的方法。在这些最近的研究中，值得注意的是，输入句子的句法树（即依存树）可以为深度学习模型提供有效的信息，从而得到SOTA性能 特别是，以前的RE深度学习模型大多都利用句法树中呈现的单词连接来构造网络结构(例如，依存树上的图卷积神经网络(GCN))。不幸的是，这些模型由于训练数据的树结构可能与测试数据中的树结构有很大的不同而导致泛化性较差，即模型过拟合于训练数据的句法树结构。这一问题在跨领域RE时，尤为明显。 为克服这一问题，总体策略是获得更一般的句法树向量表示，这些嵌入向量可用来将句法信息注入到深度学习模型中，以获得更好的泛化效果。 2019年 Veyseh等人给出了RE的一般树表示，其中依存树被分解为句子中单词之间的依赖关系集（即边)，称为基于边的表示。然后，在多任务学习框架中使用这些依赖关系，来同时预测两个实体间的关系和输入句子中单词对之间的依赖关系。 基于边的表示的主要缺点是它只捕获单词之间的成对（局部）连接，而完全忽略了单词在句子中对RE问题的整体（全局）重要性，特别是，在RE的关系预测过程中，给定句子中的某些单词可能比其他单词包含更有用的信息，并且这个句子的依存树可以帮助更好地识别那些重要的单词，并为它们分配更高的重要性得分（即在两个实体之间沿最短依赖路径选择单词） 在本文中，我们提出从依存树中获得句子中每个单词的重要性得分（称为基于句法的重要性得分 syntax-based importance scores），这将作为依存树的一般向量表示，以将句法信息注入RE深度学习模型中。 如何在RE深度学习模型中使用基于句法的重要性得分？ 首先，利用深度学习模型中的单词向量表示（词嵌入可能参与训练）为每个单词计算一个重要性得分（称为model-based importance scores） 利用KL散度使上述两个重要性得分的分布一致，使这个重要性得分能够反映句法信息 为了实现上述想法，本文首次采用了ON-LSTM(Ordered-Neuron Long Short-Term Momory Networks)计算单词的model-based importance scores。 ON-LSTM相较于LSTM新填了2个门（gate）结构： master forget master input ON-LSTM局限： master gates 和 model-based importance scores 仅依赖于当前词本身及其左上下文（先前时刻隐状态） 为了更有效利用整个句子的信息，本文提出先生成整个句子的向量表示，并把其作为每个单词计算其重要性得分的输入 为了进一步改善深度学习模型的编码能力，以学习到更好的特征向量，本文引入一种新的归纳偏置： 使 基于两个实体间的最短依赖路径的特征向量 和 基于整个句子的特征向量 的相似度尽可能高 该归纳偏置基于以下直觉：句子中两个实体的语义关系可以从整个句子或者最短依赖路径中推断出来。 模型本文模型主要分为三大组件： CEON-LSTM （context-enriched ON-LSTM） 用于计算model-based importance scores syntax-model consistency component 促使syntax-based与model-based重要性得分分布一致 Sentence-Dependency Path Similarity 基于句子整体的特征向量和基于最短依存路径的特征向量的相似度组件 嵌入层三部分的拼接： 预训练词嵌入（pre-trained word embedding） 基于相对距离的位置嵌入（position embedding） t-s、t-o BIO实体标注嵌入（entity type embedding） CEON-LSTMLSTM公式如下： 计算流程图如下： ON-LSTM 为了将依存树中的句法结构融合进LSTM，引入两个门（gate）：master forget($\\tilde{f_t}$)、 master input($\\tilde{i_t}$) 首先定义累加函数 cumsum([x_1,x_2,...,x_n]) = [x_1, x_1+x_2, x_1+x_2+x_3,..., x_1+x_2+...+x_n]定义数值软化函数 $cummax(X) = cumsum(softmax(X))$ 具体计算公式如下： \\tilde{f_t} = cummax(W_{\\tilde{f_t}}x_t+U_{\\tilde{f_t}}h_{t-1}+b_{\\tilde{f_t}})\\tilde{i_t} = 1-cummax(W_{\\tilde{i_t}}x_t+U_{\\tilde{i_t}}h_{t-1}+b_{\\tilde{i_t}})\\omega_t=\\tilde{f_t} \\circ \\tilde{i_t} (按位相乘，表示重叠交汇部分)c_t = \\omega_t \\circ (f_t \\circ c_{t-1}+ i_t \\circ \\hat{c_t}) + (\\tilde{f_t}-\\omega_t) \\circ c_{t-1} + (\\tilde{i_t}-\\omega_t) \\circ \\hat{c_t}整体计算流程如下： $\\tilde{f_t}$和$\\tilde{i_t}$的输出分别从0递增到1、从1递减至0，且都可以分为两个段（近0段、近1段），其中近1段可以理解为当前时间步处于活跃状态的cell states中的神经元/维度。详见2 因此，本文利用近1段，即处于活跃态的维度的个数计算model-based scores。 master forget的值： \\tilde{f_t} = \\tilde{f_{t1}},\\tilde{f_{t2}},...,\\tilde{f_{tD}}model-based score： mod_t = 1 - \\sum \\nolimits_{i = 1..D} \\tilde{f_{ti}}CEON-LSTM ON-LSTM引入句子上下文信息 用下述$x’_t$代替mater gates计算中的$x_t$ x = x_1, x_2,..., x_Nx'_t = \\sum \\nolimits_i \\alpha_{ti} (W_x x_i + b_x)\\alpha_{ti} = \\frac{ \\exp ((W_h h_{t-1} + b_h) \\bullet (W_x x_i + b_x))} { \\sum \\limits _{j = 1} ^N \\exp ((W_h h_{t-1} + b_h) \\bullet (W_x x_j + b_x)) }Syntax-Model Consistency首先，计算syntax-based importance score ($syn_t$)： 计算依存树最短依赖路径 $DP$ (词序列) 计算依存树任意词对间最长路径长度 $T$ $ syn_t = T - minLength(w_t, DP) $ ,其中$minLength(w_t, DP)$表示计算$w_t$与最短依存路径$DP$上某一单词在依存树上的路径长度的最短长度 经过上述计算，位于最短依存路径DP上的单词的重要性得分均为T, 而其他单词，距离DP越远得分越低。 该syntax-based score可以看作是原始依存树的泛化（relaxd）版，可以避免模型对训练数据语法结构的过拟合 Syntax-Model Consistency的思路可以理解为： 利用syntax-based score为model-based score提供监督信号，通过KL散度定义loss 计算公式如下： \\overline{mod}_1, \\overline{mod}_2, ..., \\overline{mod}_N = softmax(mod_1, mod_2,...,mod_N)\\overline{syn}_1, \\overline{syn}_2, ..., \\overline{syn}_N = softmax(syn_1, syn_2,...,syn_N)L_{importance} = - \\sum \\nolimits_i \\overline{mod}_i \\log \\frac {\\overline{mod}_i} {\\overline{syn}_i}Sentence-Dependency Path Similarity 最大化基于句子整体的特征向量和基于最短依存路径的特征向量的相似度 首先划定两个单词的序列：整个句子词序列、最短依存路径词序列 max-pooling模型CEON-LSTM的隐状态序列$h = h_1,h_2,…,h_N$的上述单词序列对应的隐状态序列 首先计算句子$X$和最短依存路径$DP$的特征向量表示$R_X$、$R_{DP}$ R_X = MaxPooling_{x_i \\in X}(h_i)R_{DP} = MaxPooling_{x_i \\in DP}(h_i)定义损失函数： L_{path} = 1 - \\cos (R_X, R_{DP}) ,其中\\cos表示余弦相似度Output &amp; Loss用于RE预测的全部特征向量为： V = [x_s, x_o, h_s, h_o, R_X], s和o为主客两个实体的索引output= P(.| X, x_s, x_o) = softmax(W_{output} V + b_{output})L_{label} = - \\log P(r|X, x_s, x_o), r是最适关系标签,即真实标签损失函数定义： L = L_{label} + \\alpha L_{importance} + \\beta L_{path}参考链接[1].Exploiting the Syntax-Model Consistency for Neural Relation Extraction [2].ORDERED NEURONS:INTEGRATING TREE STRUCTURES INTO RECURRENT NEURAL NETWORKS document.querySelectorAll('.github-emoji') .forEach(el => { if (!el.dataset.src) { return; } const img = document.createElement('img'); img.style = 'display:none !important;'; img.src = el.dataset.src; img.addEventListener('error', () => { img.remove(); el.style.color = 'inherit'; el.style.backgroundImage = 'none'; el.style.background = 'none'; }); img.addEventListener('load', () => { img.remove(); }); document.body.appendChild(img); });","tags":[{"name":"Relation Extraction","slug":"Relation-Extraction","permalink":"http://hanrd.tech/tags/Relation-Extraction/"},{"name":"ACL 2020","slug":"ACL-2020","permalink":"http://hanrd.tech/tags/ACL-2020/"}]},{"title":"Scrapy使用笔记","date":"2020-08-27T06:13:55.000Z","path":"/posts/20200827a/","text":"1.创建项目 scrapy startproject tutorial 创建名为tutorial的项目 2.定义Item Items.py: import scrapy class DmozItem(scrapy.Item): title = scrapy.Field() link = scrapy.Field() desc = scrapy.Field() 3.编写爬虫Spider tutorial/spiders/下创建 dmoz_spider.py: 必须继承scrapy.Spider类；定义 name、start_urls、parse() import scrapy class DmozSpider(scrapy.spiders.Spider): name = \"dmoz\" allowed_domains = [\"dmoz.org\"] start_urls = [ \"http://www.dmoz.org/Computers/Programming/Languages/Python/Books/\", \"http://www.dmoz.org/Computers/Programming/Languages/Python/Resources/\" ] def parse(self, response): filename = response.url.split(\"/\")[-2] with open(filename, 'wb') as f: f.write(response.body) 4.爬取数据 首先，进入项目根目录 scrapy crawl dmoz 5.提取Item: Xpath 和 css 选择器selectorfor sel in response.xpath('//ul/li'): title = sel.xpath('a/text()').extract() link = sel.xpath('a/@href').extract() desc = sel.xpath('text()').extract() print title, link, desc 6.使用Itemfrom tutorial.items import DmozItem def parse(self, response): for sel in response.xpath('//ul/li'): item = TutorialItem() item['title'] = sel.xpath('a/text()').extract() item['link'] = sel.xpath('a/@href').extract() item['desc'] = sel.xpath('text()').extract() yield item 7.保存爬取的数据 scrapy crawl dmoz -o items.json document.querySelectorAll('.github-emoji') .forEach(el => { if (!el.dataset.src) { return; } const img = document.createElement('img'); img.style = 'display:none !important;'; img.src = el.dataset.src; img.addEventListener('error', () => { img.remove(); el.style.color = 'inherit'; el.style.backgroundImage = 'none'; el.style.background = 'none'; }); img.addEventListener('load', () => { img.remove(); }); document.body.appendChild(img); });","tags":[{"name":"爬虫框架","slug":"爬虫框架","permalink":"http://hanrd.tech/tags/%E7%88%AC%E8%99%AB%E6%A1%86%E6%9E%B6/"},{"name":"python","slug":"python","permalink":"http://hanrd.tech/tags/python/"}]},{"title":"Kubernetes配置指南","date":"2020-07-17T12:27:04.000Z","path":"/posts/k8sConfig/","text":"Kubernetes集群安全设置 document.querySelectorAll('.github-emoji') .forEach(el => { if (!el.dataset.src) { return; } const img = document.createElement('img'); img.style = 'display:none !important;'; img.src = el.dataset.src; img.addEventListener('error', () => { img.remove(); el.style.color = 'inherit'; el.style.backgroundImage = 'none'; el.style.background = 'none'; }); img.addEventListener('load', () => { img.remove(); }); document.body.appendChild(img); });","tags":[{"name":"Kubernetes配置","slug":"Kubernetes配置","permalink":"http://hanrd.tech/tags/Kubernetes%E9%85%8D%E7%BD%AE/"},{"name":"K8s配置","slug":"K8s配置","permalink":"http://hanrd.tech/tags/K8s%E9%85%8D%E7%BD%AE/"}]},{"title":"Linux的文件权限","date":"2020-07-15T07:35:41.000Z","path":"/posts/200715a/","text":"Linux 的文件权限用户与用户组 linux最优秀的地方，就在于多用户、多任务环境 文件所有者 在多人同时使用主机的情况下，考虑每个人的隐私权及个人喜好的工作环境 用户组 在团队开发的时候尤其重要 每个账号（用户）都可以有多个用户组的支持 其他人 不属于“文件所有者及所属用户组中用户”的其他用户 root用户极其特殊，拥有最高权限 Linux用户身份与用户组记录的文件 /etc/passwd 系统上的所有账号、一般身份用户及root的相关信息 /etc/shadow 个人密码 /etc/group 所有的组名 Linux文件权限 对数据安全性及系统保护有重要意义 Linux文件属性 ls -al 显示当前目录下所有文件及目录的详细信息 -a 显示所有，包括隐藏文件（文件名以.开始的） -l 显示详细信息 ls -al 输出7列信息 -rw-r—r— 1 root root 42304 Sep 4 18:26 install.log 第一列表示这个文件的类型与权限（permission），共计10个字符 第一个字符表示文件类型 -表示文件 d表示目录 l表示连接文件 剩下九个每3个一组，分别为“文件所有者”、“同用户组”、“其他非本用户组”的权限，均含有“rwx”三个参数组合 r表示可读 w表示可写 x表示可执行 注意：这三个权限的位置不会改变，如果没有相应权限，用-占位 第二列表示有多少个文件名连接到此节点（i-node） 每个文件都会将它的权限与属性记录到文件系统的i-node 第三列表示这个文件或目录的“所有者账号” 第四列表示这个文件的所属用户组 第五列表示这个文件的大小，默认单位B 第六列表示这个文件的创建日期或者最近修改日期 ls -l —full-time显示完整时间 第七列表示该文件名 更改文件属性与权限 chgrp 改变所属用户组 chgrp users filename.log 把filename.log加到users用户组 -R 递归更改，适合于嵌套目录 chown 改变所有者 chown bin install.log -R 递归更改，适合于嵌套目录 可以用于同时修改所有者和用户组，用:隔开 如 chown hanrd:root install.log hanrd所有者，root用户组 只改用户组 chown .root install.log chmod 改变权限 数字类型 r-4、w-2、x-1，各个值相加 rwx = 7 chmod 740 filename 符号类型 u-所有者 g-本用户组 o-其他非本用户组 a-所有上述三种身份 +添加权限、-除去权限、=设置权限 如 chmod u=rwx,go=rw .bashrc chmod a+w .bashrc 目录与文件的权限意义 文件 r-可以读取文件实际内容 w-可以编辑修改文件内容，不能删除文件 x-具有被执行权限，执行成功与否与文件内容有关 目录 r-获取目录结构列表，详细信息获取不到 w-更改该目录结构列表的权限，与文件名变动有关 新建文件或目录 删除文件或目录 重命名 移动文件、目录位置 x-能否进入该目录作为工作目录 linux文件种类与扩展名 普通文件 用-表示 纯文本文件 - ASCII 二进制文件 数据格式文件，具有特殊格式，存储数据 目录 用d表示 连接文件 用l表示 扩展名 一个文件能不能被linux执行，与它的第一列的10个属性有关，与文件名后缀一点关系没有 文件名长度限制 单一文件或者目录最大容许的文件名为255个字符 完整路径名、文件名最大4096个字符 文件名尽量避免特殊字符 document.querySelectorAll('.github-emoji') .forEach(el => { if (!el.dataset.src) { return; } const img = document.createElement('img'); img.style = 'display:none !important;'; img.src = el.dataset.src; img.addEventListener('error', () => { img.remove(); el.style.color = 'inherit'; el.style.backgroundImage = 'none'; el.style.background = 'none'; }); img.addEventListener('load', () => { img.remove(); }); document.body.appendChild(img); });","tags":[{"name":"linux","slug":"linux","permalink":"http://hanrd.tech/tags/linux/"}]},{"title":"论文笔记--关系抽取概述","date":"2020-07-11T11:02:41.000Z","path":"/posts/200711ia/","text":"Neural Relation ExtractionEmbedding1.Improving Distantly-Supervised Relation Extraction with Joint Label Embedding, EMNLP/IJCNLP 2019 利用KG中结构化的三元组信息$(h,r,t)$和实体描述(WikiPedia主页中的第一段文本)，学习Label的嵌入向量表示，结合注意力机制，以选择bag内的有效句子。 EncoderSelectorClassifier document.querySelectorAll('.github-emoji') .forEach(el => { if (!el.dataset.src) { return; } const img = document.createElement('img'); img.style = 'display:none !important;'; img.src = el.dataset.src; img.addEventListener('error', () => { img.remove(); el.style.color = 'inherit'; el.style.backgroundImage = 'none'; el.style.background = 'none'; }); img.addEventListener('load', () => { img.remove(); }); document.body.appendChild(img); });","tags":[{"name":"Relation Extraction","slug":"Relation-Extraction","permalink":"http://hanrd.tech/tags/Relation-Extraction/"}]},{"title":"首次登录与在线求助","date":"2020-07-11T02:01:31.000Z","path":"/posts/927108io/","text":"X window与命令行模式的切换 命令行模式，也称为终端界面（terminal或者console） 启动的程序称为shell，linux默认的shell就是bash Linux默认情况会提供6个terminal来让用户登录，切换的方式是：[Ctrl]+[Alt]+[F1]~[F6],用户名密码登录，exit注销 这六个终端界面命名为 tty1~tty6，[Ctrl]+[Alt]+[F7]返回图形界面tty7 Linux登录模式 仅有纯文本界面 运行等级 run level 3 图形界面登录环境 运行等级 run level 5 Linux运行等级 共计7个run level 最常用到的是run level 3 与run level 5 若想要更改linux默认的登录模式，需要修改 /etc/inittab，下次重启生效 ~说明 ~代表的是用户的主文件夹，它是个变化的值 root的主目录：/root hanrd这个用户的主目录：/home/hanrd 提示符 root提示符 # 一般用户提示符 $ 执行命令 格式为: 命令 选项 参数1 参数2 … command [-options] parameter1 … 一行命令的第一个部分绝对是命令command或可执行文件 选项参数设置 -后面接选项的简写 —后面接选项的全名 特殊情况会有 + 各部分用空格分隔，不论几个空格，都按照一个空格对待 按下回车Enter开始执行 命令太长可以用反斜杠\\转义Enter，下一行续写 特别注意，linux命令大小写敏感 ls ls -al ~ ls -a -l ~ 基础命令 echo $LANG 显示目前所支持的语言 LANG=en_US 临时修改为英语 date 显示日期与时间 date +%Y/%m/%d 特定输出 cal 显示日历 cal 12 2020 指定年月 bc 显示计算器 quit离开 +、-、*、/、^、% 默认只输出结果的整数位 保留小数要执行scale=3，指定保留几位小数 常用快捷键 [Tab] 命令补全 文件补齐 [Ctrl] + c 中断目前程序/命令 [Ctrl] + d 表示键盘输入结束 可以用来直接离开文字界面，关闭terminal 相当于输入exit 在线求助 man page / info pageman page man是manual（操作说明）缩写 查询出的页面，叫做man page 如，man date man page的文件数据通常放在/usr/share/man目录里，可通过修改名为/etc/man.config的文件更改目录（不同的distribution文件名略有不同） 详细说明 诸如“DATE(1)”中的数字的意义（1-9），常用数字如下： 1—用户在shell环境中可以操作的命令或者可执行文件 5—配置文件或者是某些文件的格式 8—系统管理员可用的管理命令 man page 主要内容包括： NAME—简短的命令、数据名称说明 SYNOPSIS—简短的命令执行语法简介 DESCRIPTION—较为完整的说明，这部分最好仔细看看 OPTIONS—针对SYNOPSIS的所有的可用的选项的说明 SEE ALSO—其他参考材料 FILES—某些有关的文件 其他诸如，EXAMPLES、BUGS、AUTHORS等 man page 常用按键 向下翻一页 空格 [Page Down] 向上翻一页 [Page Up] 回到第一页 [Home] 去到最后一页 [End] 向下查询字符串 /string 向上查询字符串 ?string 继续查询 n继续当前查询方向查询写一个 N当前查询方向逆方向查询下一个 离开 q 高阶命令 查找与指定命令或数据有关的man page文件 whatis 命令或数据 需要root权限执行 makewhatis创建whatis数据库 man -f 命令或数据 查找包含指定关键字的man page文件 apropos 命令或数据 需要root权限执行 makewhatis创建whatis数据库 man -k 命令或数据 info page info与man的用途差不多 info page将文件数据拆成一个一个的段落，每个段落用自己的页面撰写，并且每个页面有“超链接”来跳到不同的页面中，每个独立的页面称为节点（Node） info page是只有linux才有的产物，必须按照info page的格式撰写求助文件才能具有info page的功能，info page的文件放置于/usr/share/info/目录中 非info page格式的文件也能用info显示，只不过显示效果与man相同 info page内容 第一行 File—文件名（xx.info） Node—节点名 Next—下一个节点 Up—回到上一层的节点总览界面 Prev—上一个节点 Menu 可使用方向键选择，按下[Enter]，前往该小节 也可[Tab]，在节点间移动 按键 空格键 — 向下翻页 [Page Down] — 向下翻页 [Page Up] — 向上翻页 [Tab] — 在节点间移动，节点以“*”显示 [Enter] — 进入该节点 B — 光标移至info界面第一个节点处 E — 光标移至info界面最后一个节点处 N — 前往下一个节点 P — 前往上一个节点 U — 向上移动一层 S或者/ — 查询搜索 H — 显示求助菜单 ? — 命令一览表 Q — 离开 上述命令不区分大小写字母 其他文档 documents /usr/share/doc目录包含一些额外的说明文件，包含how-to及相关原理说明 文本编辑器 nano 进入nano nano text.txt 离开nano [Ctrl] + X Y/N 查询字符串 [Ctrl] + W ^表示[Ctrl] M表示[Alt] 正确关机方法 关机时要注意： 查看系统的使用状态 who 查看谁在线 netstat -a 查看网络联机状态 ps -aux 查看后台执行的程序 通知在线用户关机的时刻 正确的关机命令使用 shutdown reboot 非正常关机可能造成文件系统的毁损 数据同步写入硬盘 命令 sync 目的 在默认情况下，某些已经加载内存中的数据不会直接被写回硬盘，而是先存在内存中，因而，特殊情况或者非正常关机会使数据的更新不正常 需要sync将内存里的尚未被更新的数据写入硬盘 常用关机重启命令shutdown shutdown可以完成如下工作： 可以自由选择关机模式：关机、重启、单用户操作模式 可以设置关机时间 可以自定义关机消息 可以仅发出警告消息，不关机或重启 可以选择是否要用fsck检查文件系统 shutdown — 依据目前已启动的服务逐次关闭各服务，然后关机 shutdown -h now 立刻关机 shutdown -h 20:25 20：25关机 shutdown -h +10 十分钟后关机 shutdown -r now 立即重启 shutdown -r +30 ‘the system will reboot’ 30分钟后重启，在目前登录者的屏幕前方显示该信息 shutdown -k now ‘the system will reboot’ 仅发出警告消息，系统不会关机 reboot 重启 通常执行: sync;sync;sync;reboot halt 不理会目前系统状态，硬件关机 poweroff 关机 poweroff -f 切换执行等级 init run level 0 关机 run level 3 纯命令行模式 run level 5 含有图形界面模式 run level 6 重启 init 0也可以关机 document.querySelectorAll('.github-emoji') .forEach(el => { if (!el.dataset.src) { return; } const img = document.createElement('img'); img.style = 'display:none !important;'; img.src = el.dataset.src; img.addEventListener('error', () => { img.remove(); el.style.color = 'inherit'; el.style.backgroundImage = 'none'; el.style.background = 'none'; }); img.addEventListener('load', () => { img.remove(); }); document.body.appendChild(img); });","tags":[{"name":"linux","slug":"linux","permalink":"http://hanrd.tech/tags/linux/"}]},{"title":"2.Linux如何学习、主机规划与磁盘分区","date":"2020-07-08T00:53:05.000Z","path":"/posts/927108rt/","text":"Linux操作模式 图形界面 命令行界面（Command Line） Linux与硬件的搭配 各个组件或设备在Linux下面都是一个文件 硬盘 /dev/sd[a-p] 磁盘分区 SATA/USB接口的磁盘根本没有一定的顺序 设备文件名命名时，根据Linux内核检测到磁盘的顺序命名 磁盘的组成 磁盘的第一个扇区特别重要，包括： 主引导分区（Master Boot Record， MBR） 安装引导加载程序的地方，446B 系统开机的时候会主动读取 分区表（partition table） 记录整块硬盘分区的状态， 64B 分为若干记录区（最多4个），每组记录区记录了该区段的起始与结束的柱面号码 所谓的分区只是针对64B的分区表进行设置 硬盘默认的分区表仅能写入四组分区信息，这四个分区信息称为主分区（primary）或扩展分区（extended） 扩展分区最多只能有一个 分区最小的单位是柱面 扩展分区继续切分的分区，叫做逻辑分区，逻辑分区的设备名称号码由5开始，1-4是保留给主分区或者扩展分区使用的，SATA硬盘最多11个逻辑分区（5-15） 能够被格式化后作为数据访问的分区为主分区与逻辑分区，扩展分区无法格式化 开机流程 BIOS是开机时计算机系统主动执行的第一个程序 BIOS会依据用户的设置去取得能够开机的硬盘，并读取该硬盘第一个扇区的MBR，MBR放置着最基本的引导加载程序，剩下的任务由该引导加载程序（Boot Loader）完成 引导加载程序目的是加载内核文件，剩下的由操作系统完成 Boot Loader 提供菜单：用户可以选择不同的开机选项 载入内核文件，开始操作系统 转交其他Loader：多重引导 引导加载程序可以安装在MBR，也可安装于每个分区的引导扇区（boot sector） 每个分区都有启动扇区 实际可开机的内核文件是放置在各分区内的 loader只会认识自己的系统分区内可开机的内核文件和其他loader loader可以直接指向或者间接将管理权转交给另一个管理程序 Linux安装模式下，磁盘分区的选择 Linux的所有数据都是以文件的形态呈现的 目录树结构 文件系统与目录树的关系（挂载） 挂载 利用一个目录作为进入点，将磁盘分区的数据放置在该目录下 进入点叫做挂载点 安装distributions，挂载点及磁盘分区的规划 / swap 预留一个备用的剩余磁盘空间 可以把/home、/usr、/boot、/var都单独分区 document.querySelectorAll('.github-emoji') .forEach(el => { if (!el.dataset.src) { return; } const img = document.createElement('img'); img.style = 'display:none !important;'; img.src = el.dataset.src; img.addEventListener('error', () => { img.remove(); el.style.color = 'inherit'; el.style.backgroundImage = 'none'; el.style.background = 'none'; }); img.addEventListener('load', () => { img.remove(); }); document.body.appendChild(img); });","tags":[{"name":"linux","slug":"linux","permalink":"http://hanrd.tech/tags/linux/"}]},{"title":"Linux是什么","date":"2020-07-06T12:31:51.000Z","path":"/posts/e3a70ff1/","text":"Linux是什么 Linux是一套操作系统，包括内核与系统调用接口（内核工具），提供了一个最底层的硬件控制与资源管理的完整架构。 1991年，芬兰人托瓦兹（Linus Torvalds）开发出了针对386硬件的内核原型。 Linux是GPL授权的‘Unix Like’自由软件。 Linux的内核版本 版本编号： 主版本.次版本.发行release版本-修改版本 2.6.18-92.e15 主次版本为奇数：开发版 2.5.xx 主次版本为偶数：稳定版 2.6.xx Linux distributions（Linux 发行版） 常说的Linux其实仅包括Linux内核与内核工具（文档） 在此基础上加上应用软件，并制成可完全安装的程序，就是Linux distribution Linux Kernel + Softwares + Tools(Documentations) + 可完全安装= Linux distribution document.querySelectorAll('.github-emoji') .forEach(el => { if (!el.dataset.src) { return; } const img = document.createElement('img'); img.style = 'display:none !important;'; img.src = el.dataset.src; img.addEventListener('error', () => { img.remove(); el.style.color = 'inherit'; el.style.backgroundImage = 'none'; el.style.background = 'none'; }); img.addEventListener('load', () => { img.remove(); }); document.body.appendChild(img); });","tags":[{"name":"linux","slug":"linux","permalink":"http://hanrd.tech/tags/linux/"}]},{"title":"入门","date":"2020-07-05T16:07:54.000Z","path":"/posts/b4c6216f/","text":"markdown document.querySelectorAll('.github-emoji') .forEach(el => { if (!el.dataset.src) { return; } const img = document.createElement('img'); img.style = 'display:none !important;'; img.src = el.dataset.src; img.addEventListener('error', () => { img.remove(); el.style.color = 'inherit'; el.style.backgroundImage = 'none'; el.style.background = 'none'; }); img.addEventListener('load', () => { img.remove(); }); document.body.appendChild(img); });","tags":[{"name":"markdown","slug":"markdown","permalink":"http://hanrd.tech/tags/markdown/"}]},{"title":"计算机概论","date":"2020-07-05T15:26:55.000Z","path":"/posts/92710899/","text":"计算机 接收用户输入指令与数据，经过中央处理器的数据与逻辑单元运算处理后，以产生或存储成有用的信息。 计算机硬件的五大单元 输入单元：键盘、鼠标等 CPU内部的控制单元 CPU内部的算术逻辑单元 内存：CPU实际要处理的数据都来自内存 输出单元：显示器、打印机等 CPU种类 精简指令集系统 RISC ARM系列 SPARC系列 复杂指令集系统 CISC AMD、Intel等x86架构 接口设备 主板 主板芯片组 存储设备 硬盘、光盘 显示设备 显卡 网络设备 网卡 计算机分类 超级计算机 大型计算机 迷你计算机 工作站 个人计算机 台式机 笔记本电脑 PC机架构与接口设备 x86架构主要有Intel、AMD 主板芯片组 北桥负责连接速度较快的CPU、内存、显卡等组件 系统总线 南桥负责连接速度较慢的周边接口，包括硬盘、USB、网卡等 输入输出总线（I/O） 特别注意，AMD的内存直接与CPU连接，不通过北桥 CPU 频率 不同的CPU由于微指令集不同、架构不同、每次频率能进行的工作指令数也不同，所以只能用来比较同款CPU的速度 外频 CPU与外部组件进行数据传输/运算时的速度 通常超频，就是超的外频 倍频 CPU内部用来加速工作性能的一个倍数 出厂时通常被锁定，无法更改 外频 * 倍频 = 频率 总线与位数 北桥所支持的频率：前端总线速度 2666MHz 每次传送的位数：总线带宽 32/64 总线频宽 = 总线带宽 * 前端总线速度 CPU每次能够处理解析的数据量：字组大小 字组大小可以与总线带宽不相同 通常说的计算机位数，是指字组大小 CPU等级 目前64位CPU：x86_64等级 内存 DRAM 动态随机访问内存 内存主要组件就是DRAM，断电数据消失 DDR:双倍数据传送速度 对服务器而言，内存的容量有时比CPU速度还重要 双通道设计 扩大内存数据宽度 CPU与内存的外率要尽量一致为佳 SRAM 静态随机访问内存 通常SRAM容量较小、速度频率高，用作CPU 的L2缓存，集成到CPU内部 ROM 只读存储器 非挥发性内存，断电数据不消失 BIOS程序存储在ROM中，开机首先会读取BIOS程序 固件，也是一个程序，是对硬件更加重要的部分，最初也是使用ROM来进行软件的写入，现在写入到闪存或EEPROM中 显卡 显存、3D加速芯片GPU PCI-Express（PCIe）规格 硬盘与存储设备 物理组成 盘片 机械手臂 磁头 主轴马达 数据 磁道 扇区 512B 柱面 分割硬盘的最小单位 传输接口 IDE接口 SATA接口 SCSI接口 CMOS与BIOS CMOS主要功能为，记录主板上面的重要参数，包括系统时间、CPU电压与频率、各项设备的I/O地址与IRQ等,需要主板上的电池供电 BIOS是写入到主板上某一块闪存或EEPROM的程序，它可以在开机的时候执行，以加载CMOS当中的参数，并尝试调用存储设备中的开机程序，进一步进入到操作系统中。 操作系统 两部分 内核 直接参考硬件规格生成 功能： 系统调用接口 程序管理 内存管理 文件系统管理 设备驱动 系统调用 目前操作系统都会包含内核和相关的用户应用软件 操作系统只是在管理整个硬件资源，包括CPU、内存、输入输出设备及系统文件 应用程序的开发都是参考操作系统提供的开发接口，所以该应用程序只能在该操作系统上面运行 document.querySelectorAll('.github-emoji') .forEach(el => { if (!el.dataset.src) { return; } const img = document.createElement('img'); img.style = 'display:none !important;'; img.src = el.dataset.src; img.addEventListener('error', () => { img.remove(); el.style.color = 'inherit'; el.style.backgroundImage = 'none'; el.style.background = 'none'; }); img.addEventListener('load', () => { img.remove(); }); document.body.appendChild(img); });","tags":[{"name":"linux","slug":"linux","permalink":"http://hanrd.tech/tags/linux/"}]}]